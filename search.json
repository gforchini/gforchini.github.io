[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Short CV",
    "section": "",
    "text": "Laurea in Economia e Commercio, Università di Bergamo (Italy),\nMaster of Economics from CORIPE P.te, Turin (Italy)\nMSc in Economics and Econometrics, University of Southampton (UK)\nPhD Economerics, University of Southampton (UK)"
  },
  {
    "objectID": "bio.html#education",
    "href": "bio.html#education",
    "title": "Short CV",
    "section": "",
    "text": "Laurea in Economia e Commercio, Università di Bergamo (Italy),\nMaster of Economics from CORIPE P.te, Turin (Italy)\nMSc in Economics and Econometrics, University of Southampton (UK)\nPhD Economerics, University of Southampton (UK)"
  },
  {
    "objectID": "bio.html#current-position",
    "href": "bio.html#current-position",
    "title": "Short CV",
    "section": "Current position",
    "text": "Current position\nProfessor of Econometrics, Umeå Universitet"
  },
  {
    "objectID": "bio.html#previous-positions",
    "href": "bio.html#previous-positions",
    "title": "Short CV",
    "section": "Previous positions",
    "text": "Previous positions\nProfessor of Econometrics, University of Surrey\nAssociate professor of Econometrics, Monash University\nSenior Lecturer, Monash University\nLecturer, University of York"
  },
  {
    "objectID": "bio.html#interests",
    "href": "bio.html#interests",
    "title": "Short CV",
    "section": "Interests",
    "text": "Interests\nAn econometrician by training, my interests are in all mathematical aspects of econometrics and statistics (or data science to be fashionable). Specific areas I have worked in are optimal inference, statistical decision theory, exact and asymptotic distribution theory, tests, tests for misspecification, structural equations models, identification, conditional inference, panel data models, saddlepoint approximations, higher order approximations, applications of differential geometry to statistics.\nSince January 2020 I have been collaborating with an interdisciplinary team working on pandemic and pandemic preparedness at the MRC Centre for Global Infectious Disease Analysis and WHO Collaborating Centre for Infectious Disease Modelling, School of Public Health, Imperial College London, London, UK."
  },
  {
    "objectID": "bio.html#publications",
    "href": "bio.html#publications",
    "title": "Short CV",
    "section": "Publications",
    "text": "Publications\nAvailable from Google Scholar"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "Here you will find my random thoughts on various aspects of econometrics and statistics and more …\n\n\n\n\n\n\n\n\n  \n\n\n\n\nComputing exact distributions part 1\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nComputing exact distributions\n\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2023\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nAssessing IV models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nFlipping out 2\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2019\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nFlipping out\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2019\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nThe trouble with doing statistics without understanding it\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2018\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nIdentification of Structural Vector Autoregressive Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2018\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nTests for over-identification in a single structural equation: what is what?\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2018\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nEconometrics and Data Science\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2017\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nInstallation and other things…\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2017\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nInterpreting the coefficients of the endogenous variables in IV models\n\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2017\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nA PhD thesis defense\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2017\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nThe rank condition for identification in linear simultaneous equations\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2017\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nThe Makita Drill\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2017\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nPhD Level Econometrics\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2016\n\n\nGiovanni Forchini\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2016\n\n\nGiovanni Forchini\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/CED0/index.html",
    "href": "posts/CED0/index.html",
    "title": "Computing exact distributions",
    "section": "",
    "text": "Introduction\nThe computation of exact distributions relies on a set of techniques that do not seem to be part of the modern econometrician’s toolbox. Here and in the next few blog articles will review these techniques, but before I will try to persuade the reader that this is worth doing.\nImprovements in the computer speed and memory allow a researcher to simulate almost every conceivable distribution. So, why should we try to derive these distributions analytically? There are several reasons.\nFirst, by calculating exact distributions we may be able to deduce properties of estimators and tests that are not immediately obvious. For example, we may find that the distribution of the statistics of interest depends on some specific parameters only, or on a certain function of the parameters. We can use this knowledge to improve simulations designs, and make them faster and more efficient.\nSecond, calculating exact distribution may emphasise aspects of the distribution that cannot be picked up by using simulations. For example, it was common in the econometric literature of the 1970s to compare estimators of the coefficient of the endogenous variables in linear structural equations by comparing their moments. However, for a common estimator, the LIML, these do not exist.\nThird, knowledge of the exact distribution can be very useful for generating random numbers in simulations based methods. Here the problem is that of generating random variables having a specified distribution: common algorithms, such as the “accept-reject method” and modification thereof require the knowledge of the exact distribution of the random variable one wants to generate. In the multivariate case, knowledge of some exact distribution theory can make the algorithms computationally efficient.\nExact distribution can often be used in conjunction with asymptotic theory. The weakly identified structural equations models are an example of this. Here one finds that under weak instrument the asymptotic distribution of the usual estimator of the coefficients of the endogenous variables in a structural equation model formally almost the same as their exact distributions under the assumption of normal errors.\nThe aim of this and the following few blog articles is to acquaint the reader with the main techniques that have been used or that are currently used for the evaluation of exact distributions."
  },
  {
    "objectID": "posts/flippingout2/index.html",
    "href": "posts/flippingout2/index.html",
    "title": "Flipping out 2",
    "section": "",
    "text": "In my previous post I discussed how I flipped the Master course Introduction to Mathematical Economics 1. But the main question is whether it worked.\nStudents’ feedback suggests that it worked well. Students found the material easy to learn because they could do it at their own time and pace. And they could spend the contact time practicing together with my help.\nObviously there are still parts that with some thinking and some work can be improved and other which need to be rethought through.  I have a list of things to do for next year. But, I admit to being surprised by how well the flipped classroom approach worked. I am thinking of implementing it for other courses."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the start of my blog. I am not sure I know why I want to do this. Maybe I want to rationalize my thoughts? Think through some issues? Connect to people? Maybe I am just a megalomaniac or I am going through a midlife crisis. I don’t know but hope to find out … so … here it starts…."
  },
  {
    "objectID": "posts/installation/index.html",
    "href": "posts/installation/index.html",
    "title": "Installation and other things…",
    "section": "",
    "text": "I was ‘installed’ as a professor of Econometrics at Umeå University on October 21, 2017. It was a wonderful day which made me feel valued. As part of the ‘installation’ ceremony all new professors had to give a talk about their research for the general public, participate in a formal ceremony and to a banquet in the evening.\nThe topic of my talk was Econometrics and Data Science. This was my reflection on my wider research. I wrote a post in the past where I discussed how I feel and fear that data science is done in a very uncritical way. In my talk, reported in a future post, I tried to put it into a historical context.\nIt was also great to be able to attend talks in a variety of disciplines offered by new professors, honorary doctors and prize winners. There is definitely lots of very interesting research going on.\nAfter the formal ceremony, I expected the banquet in the evening to be formal, too. But it wasn’t. I will not reveal anything about it. I do not want to spoil the surprise for future attendees…"
  },
  {
    "objectID": "posts/Interpretation/index.html",
    "href": "posts/Interpretation/index.html",
    "title": "Interpreting the coefficients of the endogenous variables in IV models",
    "section": "",
    "text": "In the simple linear model \\(y=x \\beta +u\\), the (scalar) parameter \\(\\beta\\) has a very simple interpretation:\n\\[\\frac{\\partial E\\left( y|x \\right)}{\\partial x} = \\beta. \\ \\ \\ \\ (1)\\]\nThat is, a marginal change in the independent variable \\(x\\) leads to a change \\(\\beta\\) in \\(E\\left( y|x \\right)\\). It is natural to carry forward this interpretation to a simple linear instrumental variable models of the form\n\\[y=x\\beta+u\\] \\[x=z'\\gamma+v,\\]\nand claim that \\(\\frac{\\partial E\\left( y|x,z \\right)}{\\partial x} = \\beta\\). However, this is not the case: since \\(u\\) and \\(v\\) are dependent, a change in \\(x\\) may also lead to a change in \\(E\\left(u|x,z \\right)\\), which can no longer expected to be zero.\nFor simplicity, assume that \\(\\left( u , v\\right)|z\\) are a pair of normal random variables with zero mean and covariance matrix\n\\[\\Sigma = \\left( \\begin{array}{cc}\n\\sigma_u^2 & \\rho \\sigma_u \\sigma_v \\\\\\rho \\sigma_u \\sigma_v &\\sigma_v^2\n\\end{array}\\right).\\]\nAlthough this assumption is not needed, it helps to simplify the calculations. Focus on the pair \\(\\left( u,x\\right)\\). Notice that they are normally distributed given \\(z\\),\n\\[\\left. \\left(\\begin{array}{c}\nu \\\\x\n\\end{array} \\right) \\right| z \\sim N\\left(\\left( \\begin{array}{c} 0 \\\\z'\\gamma \\end{array}\\right) , \\Sigma \\right).\\]\nSo that we can easily find the conditional distribution of \\(u\\) given \\(x\\) and \\(z\\) as\n\\[u | x ,z\\sim N\\left( \\rho \\frac{ \\sigma_u}{\\sigma_v} \\left( x-z'\\gamma\\right) ,\\left(1-\\rho^2 \\right) \\sigma_u^2 \\right).\\]\nNotice that the conditional mean of \\(u\\) depends on \\(x\\) as well and on \\(z\\) and\n\\[\\frac{\\partial E\\left( u|x,z\\right) }{\\partial x} =\\rho \\frac{ \\sigma_u}{\\sigma_v}.\\ \\ \\ \\ (2)\\]\nNow focus on the distribution of \\(y\\) given \\(x\\):\n\\[y|x,z \\sim x\\beta+u|x,z \\sim N\\left(x\\beta+\\rho \\frac{ \\sigma_u}{\\sigma_v} \\left( x-z'\\gamma\\right) ,\\left(1-\\rho^2 \\right) \\sigma_u^2 \\right). \\ \\ \\ \\ (3)\\]\nTherefore\n\\[\\frac{\\partial E\\left( y|x,z \\right)}{\\partial x} = \\beta+ \\rho \\frac{ \\sigma_u}{\\sigma_v}. \\ \\ \\ \\ (4)\\]\nUsing (2) and (4), \\(\\beta\\) can be written as\n\\[\\beta = \\frac{\\partial E\\left( y|x,z \\right)}{\\partial x} - \\frac{\\partial E\\left( u|x,z \\right)}{\\partial x}. \\ \\ \\ \\ (5)\\]\nHence, \\(\\beta\\) is the change in \\(E\\left( y|x,z \\right)\\) due to a change in \\(x\\) after removing the effect that \\(x\\) has on \\(u\\). A graph may be useful to understand the effect of \\(x\\) on \\(E\\left( y|x,z \\right)\\):\n\\[\\begin{array}{lcccl}\nx & &\\to & &v\\\\\n&&&&\\\\\n\\downarrow \\beta &&&&\\downarrow \\rho \\frac{\\sigma_u}{\\sigma_v}\\\\\n&&&&\\\\\ny& &\\leftarrow& &u\n\\end{array}\\]\nwhich shows how \\(x\\) affects \\(y\\) directly (and this direct effect is \\(\\beta\\)) and indirectly through the unobserved errors (and this effect is \\(\\rho \\frac{\\sigma_u}{\\sigma_v}\\)).\nPearl’s causal interpretation of \\(\\beta\\) is slightly different since it is based on the assumption that it is possible to change \\(x\\) by setting it equal to a particular value and removing the reduced form \\(x=z'\\gamma+v\\) from the system.\nNotice that the assumption of normality is not needed. We could have also taken the conditional expectation on both sides of the structural equation to find\n\\[E\\left( y|x,z\\right) = x\\beta+E\\left( u|x,z \\right).\\ \\ \\ \\ (6)\\]\nOne obtains the earlier result by deriving with respect to \\(x\\) both the left and the right hand sides of (6).\nI conclude this note with a few observations with which econometricians usually are not familiar:\n\nFirstly, notice that the conditional distribution of \\(y|x,z\\) in (3) is degenerate when \\(\\rho^2 \\to 1\\). Precisely, the conditional density becomes concentrated around the point \\(E\\left(y|x,z\\right) =x\\beta \\pm \\frac{ \\sigma_u}{\\sigma_v} \\left( x-z'\\gamma\\right)\\) as \\(\\rho^2 \\to 1\\).\nSecondly, what should one be interested in? the direct effect\\(\\frac{\\partial E\\left( y|x,z \\right)}{\\partial x}\\)? or the total effect \\(\\beta=\\frac{\\partial E\\left( y|x,z \\right)}{\\partial x}- \\frac{\\partial E\\left( u|x,z \\right)}{\\partial x}\\)? This may depend on the scope of the analysis.\nThirdly, if we have a sample of iid observations \\((y_i,x_i)\\) \\(i=1,2,...,N\\), as the sample size \\(N \\to \\infty\\), the OLS estimator of \\(\\beta\\) tends to \\(\\beta+\\rho \\frac{ \\sigma_u}{\\sigma_v}\\). Therefore, the OLS estimator is consistent for \\(\\frac{\\partial E\\left( y|x,z \\right)}{\\partial x}\\).\nThe traditional interpretation of \\(\\beta\\) in econometrics is based on the observation that \\(E\\left(y|z\\right)=E\\left(x|z\\right) \\beta+E\\left(u|z\\right)\\) since \\(E\\left(u|z\\right)=0\\) it follows that \\(\\beta = \\frac{E\\left(y|z\\right)}{E\\left(x|z\\right)}\\)."
  },
  {
    "objectID": "posts/EtrixDataS/index.html",
    "href": "posts/EtrixDataS/index.html",
    "title": "Econometrics and Data Science",
    "section": "",
    "text": "Unless you have a degree in economics or business you are unlikely to know what econometrics is. Econometrics is a set of statistical methodologies used to link economic models with economic data. As such econometrics provides a set of tools which are used in decision making by governments, central banks, charities, financial investment agents, advertising agencies, etc.\nData science is the “sexiest job of the 21st century” (at least according to an article published in the Harvard Business Review in October 2012). Maybe that is why several of my colleagues turned overnight into data scientists. The term data science has been around for over 50 years. It was used from the 1960 as a synonym for computer science. It was equated to statistics by Professor Jeff Wu in 1997, but now it is usually thought of as having a much broader role than statistics with a great emphasis on computing.\nSearching the internet one finds “romantic” definitions of data science. For example a data scientist is an individual who can mix a bit programming, a bit of hacking, a bit statistics and good visualizations but is not an expert in any of its individual aspects. Or, data science is what let the data speak! Or, in the words of Sean Rad, the founder of Tinder: “Data beats emotions.”\nThe data scientist is an “alchemist”, a data tinkerer who gets things done without having an advanced degree. In fact, several internet sites for aspiring data scientists advise data scientists not to get advanced degrees because “they get in the way”. Most competitors on the platform Kaggle - the self proclaimed home of data science and machine learning - are self taught.\nDespite dealing with variables which are often economic in nature, data scientists have a background in science - physics, computer science, and engineering. They use a lot of new techniques which are slowly finding their way into Econometrics. This is certainly a good thing. More openness of Econometrics is a positive and welcome development.\nBut can data really speak on its own? Economists always spoil a party with cynical old quotes like:\n“Torture the data, and it will confess to anything.”\nsays Ronald Coase, Economics, Nobel Prize Laureate.\nThese warnings are based on experience but are usually silenced by buzzwords and the apparent unstoppable progress brought about by technology. So let’s put the buzzwords aside and go back to the 30’ before the advent of computers as we know them. In fact, human computers were used at that time.\nIn 1936 Jan Tinbergen was commissioned by the League of Nations to test the business cycle theory, and in 1939 he published a report “Statistical Testing of Business-Cycle Theories” which started an interesting and long lasting debate among economists.\nSince the nineteenth century, economists have noticed a cyclical pattern in economic activity, and developed explanations sometimes appealing to physical phenomena like the cycle of the sun. Tinbergen, a former physicist, who switched to economics because he found it more socially useful, set out to build a quantitative macroeconomic model explaining the business cycle which, he hoped, could be useful to relieve the great depression.\nTinbergen realized that economics wasn’t specific enough to fully specify a system of causal relationships which could be estimated from data. In particular, economics was totally uninformative about the way expectations were formed - and expectations are fundamental for economic decisions - and about the temporal effects, the lagged effects, of changes in policies. So, he tinkered. He created variables approximating expectations and lagged effects, and did what data scientists now call “features engineering”. He was using state of the art techniques and a lot of calculations - done using human computers - to estimate and test different and complex models. He had the spirit of a data scientist and he was ready to employ extensive computational power to produce his results.\nTinbergen’s report for the League of Nations was widely circulated before publications and John Maynard Keynes was asked to provide a critique of Tinbergen’s methods. Keynes was a famous British economist whose influence in economics has been vast. He was a former mathematician who also wrote a book on probability.\nKeynes was quite a harsh critic. Some of his points were trivial but others were not and are still relevant today. Keynes took issue, although with a flowery prose, with matters which are familiar to econometricians today including dynamic specification, structural change, simultaneity bias, measurement error, omitted variable bias, spurious correlations etc. If any of these is neglected, inference becomes invalid.\nSome of the examples where things go wrong are quite technical and are not suitable for a general talk.\nHowever, some are not. For example, one of the point Keynes raised was that a relationship estimated with a data from a given historical period can be used for predictions or in general for modelling the relationship in another historical period only if this relationship holds in both periods. What Keynes was underlying was that this is an assumptions which needs not be true in economics because economic agents learn from the past. A recent example where this assumption fails is Google prediction of flu cases in 2008. Google researchers published a paper in Nature claiming that they could accurately forecast flu prevalence in the US two weeks earlier that the Centers for Disease Control and Prevention. However four years later the same methodology failed to estimate the peak of the flu season by quite a lot.\nAnother point was that if economics does not lead the analysis then one would pick up spurious relationships, relationships which seem to exist but do not. Keynes was a firm believer that deductions from economic theory on the causal mechanisms of economic phenomena should fully inform data analysis. What Tinbergen was doing was “alchemy” in his view.\nA third point was that economic variables used as explanatory variables are usually imprecise measures of some ideal economic quantities. The error with which these variables are measured affects what we can learn from them.\nI like a quote from the Keynes’ review of Tinbergen report:\n“I hope I have not done injustice to a brave pioneer effort. The labour it involved must have been enormous. The book is full of intelligence, ingenuity, and candour; I leave it with sentiments of respect for the author.\nBut it has been a nightmare to live with, and I fancy that other readers will find the same. I have a feeling that Prof. Tinbergen may agree with much of my comments, but that his reaction will be to engage another ten computors and drown his sorrows in arithmetic.”\nAlready in 1938 we could find data scientists - Tinbergen - and economists - Keynes - inside what will become econometrics. As with any debate, support went to both sides of the arguments. There were Nobel laureates on both sides.\nThis early debate help shape the development of econometrics, leading to a fundamental contribution in 1943 by Trygve Haavelmo. Haavelmo had good grounding in statistics.\nHaavelmo contribution was twofold. Firstly he regarded the economic variables as random variables, stochastic objects that is. This means that the object of statistical inference are the parameters of the probability laws generating the data. This establishes a connection between economic theory and data.\nSecondly, Haavelmo argued that “we shall not, by logical operations alone, be able to build a complete bridge between our model and reality”.\nThis means that a model is a description of reality not the reality in the same way as Magritte’s picture of a pipe is not pipe. A model can be interpreted but the interpretation requires more than the model itself. This is a paradigm where “Theories with different economic meaning might lead to exactly the same probability law”. It is therefore important to understand under what conditions an interpretation is unique. This is a topic which I find very interesting and spent a considerable amount of time investigating.\nThis has been the paradigm of econometrics for a few years. It is a paradigm in which economic theory and statistics played different but complementary parts and in which causality does not exist. It is only much later that a discussion started on whether econometric or statistical models can be used to infer causation. But this is a different story.\nThis standard approach has also ward off a fully data based approach: data analysis needs to be directed by economics principles. Data cannot speak on its own. It speaks through economics.\nAlthough the standard approach is considered as the gold standard, Economists have always been open to all kinds of methodologies and approaches to analyse data. So much so that for example an article in the Economist warned in 2016 that “Economists are prone to fads, and the latest is machine learning” (The Economist, Nov 24th, 2016).\nEconometrics is mature enough to incorporate new ideas and techniques, without accepting these in an uncritical and subservient way, but modifying and adapting such methodologies to an economic context. After all, economic variables reflect agents’ decisions rather than immutable laws. Tinkering is allowed, a bit alchemy too. But they are directed and controlled by economics.\nSo what is the sexiest job of the 21st century? Econometrics, obviously.\n(This is the content of my talk as part of the ‘installation’ of new professor at Umeå University on October 21, 2017)."
  },
  {
    "objectID": "posts/flippingout/index.html",
    "href": "posts/flippingout/index.html",
    "title": "Flipping out",
    "section": "",
    "text": "My first attempt at flipping Mathematical Economics 1 starts today. This is a unit for Master students - obviously - in economics covering basic calculus, linear algebra, matrices, constrained and unconstrained optimisation, and a bit of differential and difference equations. The preparation took a fair amount time mostly due to the need of finding the right technology for doing this. I decided to use on PowerPoint because I can easily record my voice on the slides, keep the timing of the presentation and create a film. In my first attempt, I tried to record the screen of my laptop while writing on a tablet. This was cumbersome, my handwriting was not good and the graphs funny and imprecise.\nPowerPoint allows me to record short lectures with clear equations and graphs without any complicated technology. Google presentations does not support this feature so was quickly excluded.\nInformation for the students, including references and links to material, are provided using Google Docs. They are easy to update and share.\nOnline quizzes are essential to identify what needs to be practiced in the classes. Quizzes were and are a problem! In the end, I used Google Forms, but they are far from ideal. The problem is that I cannot type maths in Google Forms and need to use an elaborate procedure: I use an external maths editor to create a picture of an formula which is then included in Google Forms. Since pictures cannot be inserted in-line the questions are often unnatural. It is also easy to make mistakes and laborious to correct them. A Microsoft alternative to Google Forms allows for writing better equations - not in-line - but is much more constraining in its use. Other alternatives look very flashy but are pretty useless for mathematical quizzes in my view.\nShort lectures ranging from 1m to 20m are uploaded on YouTube as unlisted videos - I am too much of a wimp to make them public.\nI will shortly find out whether this approach to teaching Mathematical Economics 1 works …"
  },
  {
    "objectID": "posts/PhDEconometrics/index.html",
    "href": "posts/PhDEconometrics/index.html",
    "title": "PhD Level Econometrics",
    "section": "",
    "text": "In the summer 2016 I was asked to contribute to the teaching of the first year MRES unit Econometric Theory at University of Warwick. The first term - my part - covered micro econometrics while the second semester is about time series.\nThe MRES forms the first two taught year of a PhD programme in economics. All the students taking it had a background in economics and they had already seen a fair bit of econometrics at undergraduate or MSc level. I thought a lot about what the first semester of this unit should cover.\nI wanted this unit to:\nprepare the students for applied research in economics; give the students an understanding of econometrics which allows them to get the gist of research papers; cover modern topics.\nI did not want this unit to be a repeat, at higher level, of what they have already seen several times.\nTherefore,I decided to cover a bit of asymptotic theory at the start as it underpins most of modern econometrics. This was done with a lot of references to the linear regression model which should be familiar to the students, and wasfollowed by the method of moments and by the generalised method of moments with a lot of examples that the students should already have encountered in their studies.\nSome panel data models followed. Here, the the students could practice the use of the asymptotics as well as possibly revise the standard estimators for these models. The Anderson-Hsiao and the Arellano-Bond estimators were interesting examples of the generalised method of moments.So far the topics covered werefairly standard.\nAt this point I decided to introduce some nonstandard topics. I started inference in panel data with common shocks, because they represent a very plausible practical situation. This was followed by quantile regression since it has important practical applications. Then I discussedsome methods for model selection, shrinkage and model reduction. These topics are often brushed aside in econometrics and negatively branded as ‘data mining’. Nonetheless, techniques like the lasso are starting to receive interest from econometricians.\nThe final topic was missing observations since this is a situation the students will certainly come across in their careers as economists. I discussed various forms missing observations and ways to get around them: complete cases, imputation and various forms of sample selection.\nThere are lots of interesting topics I had to leave out due to lack of time. These includes among several others: semi- and non-parametric statistics, program evaluationandcausal graphs."
  },
  {
    "objectID": "posts/The Makita drill/index.html",
    "href": "posts/The Makita drill/index.html",
    "title": "The Makita Drill",
    "section": "",
    "text": "“Data Scientist: The Sexiest Job of the 21st Century” was the title of an HBR article from 2012. It is unclear what a data scientist is, but it seems to be an individual who uses data to solve problems. In this sense, econometricians are data scientists.\nUsing data to solve problems involves the use of mathematics, statistics, programming, quantitative reasoning and initiative. Although many people decide to adopt the title of ‘data scientist’, I believe these skills are and will continue to be in short supply for the foreseeable future, at least based on my experience in the UK and Australia.\nIn the 20th Century, one would generally teach statistics and econometrics using mathematical tools such as matrices, vectors, linear algebra, calculus, etc. These are now rarely used outside PhD level. Knowledge of mathematics and technical concepts among statistics and econometrics students is very limited. Explanations are often replaced by ‘hand waving’. Intuition, which used to supplement the understanding of a result or a technique, has become the ‘understanding’ - some of my more inquisitive students have found the lack of detail in the treatment of key results frustrating. In addition, the mathematics used is often taught in a very procedural way which does not emphasise conceptual understanding. In the past, universities used to encourage problem solving, but this is often no longer considered a priority. Teachers are pressured not to fail students and to make sure that a large proportion of students is awarded top marks. Learning to solve problems involves both effort and possible failure and this is no longer acceptable - see the THES article “Prepare students ‘to fail’ so they can learn, report suggests”. In a 2012 RSA report, it is stated that “English universities are sidelining quantitative and mathematical content because students and staff lack the requisite confidence and ability”. Students are, therefore, often not enabled to build on their technical knowledge and to tackle non-routine problems.\nThis tendency to avoid technical details appears to be becoming more pervasive in society as a whole. The same 2012 RSA report remarked that in Japan and China, more than 50% of degrees are awarded in STEM subjects, but this percentage reduces to less than 25% in the UK and only 16% in the US. Similar concerns have been raised in Australia. An article in THES in December 2016 emphasises that ‘reading popular science articles causes non-scientists to overrate their expertise’. LinkedIn groups on data science are full of articles with fancy titles, lots of waffle and little substance that are authored by individuals who self proclaim to be evangelists, pioneers, leaders, champions etc. Similarly, job specs for data scientist are full of the latest buzzwords but lack content. Skills required are often listed as R, SAS, Python, Tableau, etc. However, these are not skills but tools - it is as if a builder has the skills to use a Makita drill, but not a different brand. If one understands concepts and methods, it takes close to no time to choose the right tool for the job and to learn the correct way of using it. On the other hand, knowing how to use a tool does not guarantee that the analysts know what they are doing: the builder may know how to operate the Makita drill but may have no clue about where to drill or what drill bits to use, and, as we all know, this leads to expensive mistakes and wasted time.\nA simple look at any Kaggle competition shows that many competitors can reach good positions in the leaderboard by arbitrarily changing a few values in a previously publicly shared procedure. They can use the tools, but they are just guessing where to drill. By chance, they may find a wall stud but they may also drill through a water pipe or an electric cable or may irresponsibly fix a heavy load bearing shelf on a plasterboard wall. Do these individuals have the skills of a data scientist? Are these the type of data scientists that add value to a company, a business, a research project or society in general? Is this the type of data science we should teach and encourage? Is this type of data science really the sexiest job of the 21st Century?"
  },
  {
    "objectID": "posts/AssessingIV/index.html",
    "href": "posts/AssessingIV/index.html",
    "title": "Assessing IV models",
    "section": "",
    "text": "Endogeneity is a big problem in econometric modelling and as such it is usually treated extensively in intermediate/advanced econometric courses at master level. In my lectures I tend to emphasis the importance of using theory to make informed choices.\nIn the limited information approach one specifies a structural equation of the form\n\\[y_{1i}=y_{2i} \\beta + z_{1i}\\gamma+u_i\\]\nwith a corresponding reduced form for the explanatory endogenous variables \\(y_2\\) - often referred to as the first stage regression by applied economists -\n\\[y_{2i}=z_{1i} \\Pi_{1} +z_{2i} \\Pi_{2} +v_i.\\]\nIn my notation the \\(y\\)s denote the endogenous variables and the \\(z\\)s the exogenous variables of which \\(z_{2i}\\) is the vector of instruments. The observables have the following dimensions: \\(y_{2i}\\) is \\(1 \\times p\\), \\(z_{1i}\\) is \\(1 \\times k_1\\) and \\(z_{2i}\\) is \\(1 \\times k_2\\)\nThe structural parameters can be estimated using instrumental variables methods. The estimation methods do not pose many problems for the students but the assessment of the reliability of the model estimated sometimes does. Assume that \\(y_{2i}\\) is one-dimensional to avoid complications (will make a few comments only about the general case).\nThe problem is: how well can one trust the estimated model?\nThree tests are usually employed by econometricians to this purpose: a test for endogeneity, a test for strength of the instruments and a test for over-identification. These need to be performed in a certain order to be informative.\n\nTesting the strength of the instruments\nThe structural parameters are identified if \\(\\Pi_2\\) has full column rank - in the lectures we usually look at this from a couple of different points of view. Testing that \\(\\Pi_2\\) has full column rank is essentially testing for identification. This is the starting point of the evaluation because if the structural parameters are not identified there is no point in assessing whether they can be estimated well - because they cannot be estimated at all.\nIn the case where there is only one endogenous variables among the regressors, \\(\\Pi_2\\) has full column rank if it is not a zero vector. So in order to assess the identification of the model one can test \\(H_0: \\Pi_2=0\\) versus \\(H_1: \\Pi_2\\ne 0\\). This can be done using an \\(F\\) test in the first stage regression. However, one does not usually use the classical critical values because weak instruments may also cause problems similar to those caused by lack of identification. One has to be a bit more conservative. Staiger and Stock (Econometrica, 1997) have suggested a rule of thumb:\n\nIf \\(F &gt;10\\) conclude that the instruments satisfy the classical assumptions and are thus well behaved; however if \\(F&lt;10\\) than weak instruments may be present invalidating any kind of asymptotic inference.\n\nThis is a crude rule of thumb and has been improved in more recent work. Notice that if there is more than one endogenous variables among the regressors, the F statistics need to be replaced by a multivariate version (usually the Cragg-Donald statistic which is used to test the rank of \\(\\Pi_2\\)).\nThe reason why this test is performed first is that the condition \\(\\Pi_2\\) has full rank is necessary and sufficient for the various estimators of the structural parameters to be informative and for various statistics to have standard distributions. Unless there is clear evidence that \\(\\Pi_2\\) has full rank and is bounded away from zero, the asymptotic of most statistics - including the endogeneity and overidentification tests discussed below - breaks down.\n\n\nTesting endogeneity\nIf the model passes the previous hurdle, one should then test for endogeneity. This is normally done using the Hausman test - which is not robust to heteroskedasticity - or more often the Wu test - as this can be robustified to heteroskedasticity.\nOnce one conditions on the exogenous variables, including the instruments, the correlation between \\(y_{2i}\\) and \\(u_i\\) is the same as the correlation between \\(u_i\\) and \\(v_i\\). Once can think of modelling it as \\(u_i=v_i \\gamma+\\varepsilon_i\\). If \\(v_i\\) were observable we could test for endogeneity by testing\\(H_0:\\eta=0\\) in\\(y_{1i}=y_{2i} \\beta + z_{1i}\\gamma+ v_i \\eta +\\varepsilon_i.\\)\nTo overcome the fact that \\(v_i\\) is not observable, the Wu test is implemented as follows:\nfirst one estimates the first stage regression using OLS and calculate the residuals \\(\\hat v_i\\); the residuals are inserted into the structural equation which is now estimated using OLS \\(y_{1i}=y_{2i} \\beta + z_{1i}\\gamma+\\hat v_i \\eta +\\varepsilon_i;\\) and finally one tests whether the F test on the coefficients of these reduced form residuals is significant.\nDespite what its name suggests, the null hypothesis is that these coefficients are zero, that is \\(y_{2i}\\) is exogenous - versus the alternative that they are not - \\(y_{2i}\\) is endogenous.\nNotice that the first stage regression is important in testing for endogeneity, too. If it shows that the instruments are weak, the reliability is this test is undermined. In the special case where \\(\\Pi_2=0\\),\\(y_{2i}=z_{1i} \\hat \\Pi_1 +\\hat v_i\\), so that the model in 2 has perfect multicollinearity. Similar problem arise when \\(\\Pi_2\\) has rank less than the number of right-hand-side endogenous variables.\n\n\nTesting overidentifying restrictions\nThese are test for the null hypothesis that $H_0:E(u_i )=0 $ against $H_1:E(u_i ) $. It is sometimes called an orthogonality test. A previous entry discusses the different types of tests for over-identification that one can have.\nA test is based on the sample counterpart of \\(E\\left( \\left[ z_{1i},z_{2i}\\right]u_i \\right)\\):\n\\[\\frac{1}{N} \\sum_{i=1}^N z_i u_i .\\]\nNotice if the orthogonality condition fails \\(\\frac{1}{N} \\sum_{i=1}^N z_i u_i \\to^P \\sigma_{zu}\\) but if it holds \\(\\frac{1}{N} \\sum_{i=1}^N z_i u_i \\to^P 0\\).\nObviously one cannot base a test on this because the errors are not observed. However, one can replace the errors with the TSLS residuals and consider the statistic $m= _{i=1}^N z_i u_i $ instead. One can show that under some regularity conditions\n\\[\\sqrt{N} \\bar m \\to^D N\\left( 0, Asy.Cov(\\bar m)\\right)\\]\nand that once can find an estimator of \\(Asy.Cov(\\bar m)\\). Thus a Wald test can be based on:\n\\[N \\bar m ' \\left( \\widehat{ Asy.Cov(\\bar m) }\\right) ^{-1}\\bar m \\to \\chi _{k_{z_2}-p}^2\\]\nwhere \\(\\widehat{ Asy.Cov(\\bar m) }= \\frac{1}{N} \\sum_{i=1}^N \\hat u_i^2 z_i z_i'\\).\nNotice that among the regularity conditions mentioned above include the assumption that the parameters of the model are identified so that the rank of \\(\\Pi_2\\) is \\(p\\). If \\(\\Pi_2\\) is rank deficient the distribution of \\(N \\bar m ' \\left( \\widehat{ Asy.Cov(\\bar m) }\\right) ^{-1}\\bar m\\) will not converge to a \\(\\chi _{k_{z_2}-p}^2\\) but to some other distribution.Finally notice in order to be able to perform this test one requires \\(k_{z_2}-p \\ge 1\\).\n\n\nFinal remarks\nFor the reasons outlined above, one usually perform the three tests in the following order: (1) Test for the strength of the instruments; (2) Test of endogeneity and (3) tests for over identification.\nIf the test for the strength of the instruments indicates problems we need to doubt the validity of the subsequent tests.\nIf the instruments are good, but the second tests indicate that te potentially endogenous variables are in fact exogenous, one really needs to think very carefully about what to do next. It may be that the economic theory on the basis of which endogeneity is suspected may be wrong. Or it may be that some other assumptions made is wrong. So think carefully about the validity of the assumptions.\nIf the first two tests are passed but overidentification fails, one needs once again to think about what goes wrong. Are the instruments really exogenous? Is the model well specified?\nThinking is fundamental …"
  },
  {
    "objectID": "posts/overidtests/index.html",
    "href": "posts/overidtests/index.html",
    "title": "Tests for over-identification in a single structural equation: what is what?",
    "section": "",
    "text": "There is a large number of tests for over-identification for a single structural equation and there seems to be confusion about what test is what and who suggested what. This note tries to clarify this.\nConsider a structural equation of the form\n\\[{y_1} = {Y_2}\\beta + {Z_1}\\gamma + u \\ \\ \\ \\ (1)\\]\nwith reduced form\n\\[\\left[ {{y_1},{Y_2}} \\right] = {Z_1}\\Phi + {Z_2}\\Pi + \\left[ {{v_1},{V_2}} \\right] \\ \\ \\ \\ (2)\\]\nand denote by \\(\\hat \\beta\\) the TSLS estimator of \\(\\beta\\), and by \\(\\hat u\\) the TSLS vector of residuals of the regression of \\({y_1}\\) on \\(\\left[ {{Y_2},{Z_1}} \\right]\\) with instruments \\({Z_2}\\). Also write \\(Z = \\left[ {{Z_1},{Z_2}} \\right]\\) and define the usual projection matrix \\({M_A} = {I_T} - A{\\left( {A'A} \\right)^{ - 1}}A'\\) for any full column rank matrix \\(A\\). Notice \\(y_1\\) is \\(T \\times 1\\), \\(Y_2\\) is \\(T \\times n-1\\), \\(Z_1\\) is \\(T \\times k_{1}\\) and \\(Z_2\\) is \\(T \\times k_2\\). The dimension of all other terms in (1) and (2) can be easily obtained.\nIf one assumes that the rows of \\(\\left[ {{v_1},{V_2}} \\right]\\) are i.i.d. with zero mean vector and covariance matrix $$ then the structural variance is \\({\\sigma ^2} = \\left( {1, - \\beta '} \\right)\\Omega \\left( {1, - \\beta '} \\right)'\\). The TSLS estimator of the structural variance is denoted by \\({\\hat \\sigma ^2} = \\hat u'\\hat u/T\\). However one can estimate this also as \\({\\tilde \\sigma ^2} = \\left( {1, - \\hat \\beta '} \\right)\\hat \\Omega \\left( {1, - \\hat \\beta '} \\right)'\\) where \\(\\hat \\Omega = {T^{ - 1}}\\left[ {{y_1},{Y_2}} \\right]'{M_Z}\\left[ {{y_1},{Y_2}} \\right]\\).\n\nAnderson-Rubin test\nThis is the first test for over-identification and was proposed by Anderson and Rubin (1949) and is based on the statistic\n\\[\\mathop {\\min }\\limits_\\beta \\frac{{\\left( {{y_1} - {Y_2}\\beta } \\right)'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\left( {{y_1} - {Y_2}\\beta } \\right)}}{{\\left( {{y_1} - {Y_2}\\beta } \\right)'{M_Z}\\left( {{y_1} - {Y_2}\\beta } \\right)/T}} = \\frac{{\\left( {{y_1} - {Y_2}{{\\hat \\beta }_{LIML}}} \\right)'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\left( {{y_1} - {Y_2}{{\\hat \\beta }_{LIML}}} \\right)}}{{\\left( {{y_1} - {Y_2}{{\\hat \\beta }_{LIML}}} \\right)'{M_Z}\\left( {{y_1} - {Y_2}{{\\hat \\beta }_{LIML}}} \\right)/T}} \\ \\ \\ \\ (3)\\]\nwhere \\({\\hat \\beta _{LIML}}\\) is the LIML estimator of the structural equation in (1) with reduced form (2)\n\n\nSargan Test\nThis test was proposed by Sargan (1958) and uses as test statistic \\(T{R^2}\\) where \\({R^2}\\) is the coefficient of determination of the regression of \\(\\hat u\\) on \\(Z = \\left( {{Z_1},{Z_2}} \\right)\\) which is, apart from a constant of proportionality,\n\\[\\displaystyle\\frac{{\\hat u'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\hat u}}{{{{\\hat \\sigma }^2}}}. \\ \\ \\ \\ (4)\\]\nThis test is generalized to the J-test by Hansen (1982). Hausman (1983) has also suggested a test having this structure but does not refer to Sargan’s work.\n\n\nBasmann Test\nBasmann (1960) suggested a modification of the Anderson-Rubin test which uses the TSLS estimator instead of the LIML estimator. This is based on the statistic\n\\[\\frac{{\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)}}{{\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)'{M_Z}\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)/T}}. \\ \\ \\ \\ (5)\\]\nIt is worth rewriting this statistic to bring up some similarity and difference with Sargan statistic. Using the fact that \\(\\hat u = {M_{{Z_1}}}\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)\\) and that \\(\\left( {{M_{{Z_1}}} - {M_Z}} \\right){M_{{Z_1}}} = \\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\) the numerators of Sargan and Basmann statistic are equal\n\\[\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\left( {{y_1} - {Y_2}\\hat \\beta } \\right) = \\hat u'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\hat u, \\ \\ \\ \\ (6)\\]\nhowever, the denominator of Basmann test statistic is\n\\[\\begin{gathered} \\left( {{y_1} - {Y_2}\\hat \\beta } \\right){M_Z}\\left( {{y_1} - {Y_2}\\hat \\beta } \\right)/T = \\left( {1, - \\hat \\beta '} \\right)\\left( {{y_1},{Y_2}} \\right)'{M_Z}\\left( {{y_1},{Y_2}} \\right)\\left( \\begin{gathered} 1 \\hfill \\\\ - \\hat \\beta \\hfill \\\\\\end{gathered} \\right)/T \\\\ = \\left( {1, - \\hat \\beta '} \\right)\\hat \\Omega \\left( \\begin{gathered} 1 \\hfill \\\\ - \\hat \\beta \\hfill \\\\\\end{gathered} \\right). \\\\\\end{gathered}\\ \\ \\ \\ (7)\\]\nThus, Basmann and Sargan tests use alternative estimators of the structural variance.\n\n\nByron and Wegge test statistic\nHwang (1980) has shown that the test statistics suggested by Byron (1974) and Wegge (1978) are in fact the same and shows that they can be written as\n\\[\\displaystyle\\frac{{\\hat \\delta '{Z_2}{M_2}{Z_2}\\hat \\delta }}{{{{\\hat \\sigma }^2}}} \\ \\ \\ \\ (8)\\]\nwhere \\(\\hat \\delta = {\\left( {{Z_2}'{A_2}{Z_2}} \\right)^{ - 1}}Z'{A_2}y\\), \\({M_2} = {A_2} - {A_2}{Y_2}{\\left( {{Y_2}'{A_2}{Y_2}} \\right)^{ - 1}}{Y_2}'{A_2}\\) and \\({A_2} = {M_{{Z_1}}} - {M_Z}\\). Simple algebraic transformations show that\n\\[\\hat \\delta '{Z_2}{M_2}{Z_2}\\hat \\delta = \\left( {{y_1} - {Y_2}\\hat \\beta } \\right)'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\left( {{y_1} - {Y_2}\\hat \\beta } \\right) = \\hat u'\\left( {{M_{{Z_1}}} - {M_Z}} \\right)\\hat u \\ \\ \\ \\ (9)\\]\nso that the Byron and Wegge test is identical to the Sargan test. Neither Byron nor Wegge nor Hwang refer to Sargan’s work.\nThere are also some modern equivalent of these classical tests. These apply under more general conditions including for example heteroskedasticity.\n\n\nReference\n\nAnderson, T. W. and H. Rubin (1949). “Estimation of the Parameters of a Single Equation in a Complete System of Stochastic Equations.” Annals of Mathematical Statistics 21: 570-482.\nBasmann, R. L. (1960). “On Finite Sample Distributions of Generalized Classical Linear Identifiability Test Statistics.” Journal of the American Statistical Association 55: 650-659.\nByron, R. P. (1974). “Testing Structural Specification Using the Unrestricted Reduced Form.” Econometrica 42: 869-883.\nHansen, L. P. (1982). “Large Sample Properties of Generalized Method of Moments Estimators.” Econometrica 40: 1029-1054.\nHausman, J. A. (1983). “Specification and Estimation of Simultaneous Equation Models” in. Handbook of Econometrics, Volume. Z. Griliches and M. D. Intriligator. Amsterdam, North-Holland Publishing Company: 391-448.\nHwang, H.-S. (1980). “A Comparison of Tests of Overidentifying Restrictions.” Econometrica 48 (7): 1821-1825.\nSargan, J. D. (1958). “The Estimation of Economic Relationships using Instrumental Variables.” Econometrica 26: 393-415\nWegge, L. L. (1978). “Constrained Indirect Least Squares Estimators.” Econometrica 46: 435-499."
  },
  {
    "objectID": "posts/rankcondition/index.html",
    "href": "posts/rankcondition/index.html",
    "title": "The rank condition for identification in linear simultaneous equations",
    "section": "",
    "text": "The first two lectures of Econometrics B in the economics MSc at Warwick cover endogeneity and the challenges that it creates for statistical inference. In the third lecture we investigate the problem of identification. We state without proofs the rank condition for identification and apply it to several examples.\nAfter the lecture, a few students asked me where the rank condition comes from. Here is an answer (there are a few equivalent ones) … It requires a few ideas from matrix algebra.\nA general linear simultaneous equations model with \\(m\\) equations - which can be thought of as the behavioural equations - can be written as\n\\(y_i = \\Gamma z_i + u_i\\ \\ \\ \\ (1)\\)\n\\(i=1,2,...,N\\) where \\(y_i\\) is the \\(m \\times 1\\) vector of endogenous variables, \\(B\\) is the \\(m \\times m\\) matrix of coefficients of the endogenous variables, \\(z_i\\) is the \\(k \\times 1\\) vector of exogenous variables, \\(\\Gamma\\) is the \\(m \\times k\\) matrix of coefficients of the exogenous variables, \\(u_i\\) is the \\(m \\times 1\\) vector of errors. We also assume that \\(E(u_i) = 0\\), \\(var(u_i) = \\Sigma\\) where \\(\\Sigma\\) is positive definite, and \\(cov(u_i,u_j) = E(u_i u_j) = 0\\) for all \\(i \\ne j\\).\nAs we have noticed in the lecture, (1) and\n\\(y_i= F \\Gamma z_i+ F u_i \\ \\ \\ \\ (2)\\)\nhave the same reduced form\n\\(y_i= B^{-1}\\Gamma z_i+ B^{-1}u_i\\)\nfor any nonsingular matrix \\(F\\). In economics terms, these two systems of equations generate the same equilibrium values of the endogenous variables. Since we observe the equilibrium values only, we cannot distinguish between the two systems of equations generating them.\nIn order to identify the parameters of the structural equations we exploit some knowledge about the problem we want to study. This knowledge, in this case, says that the structural parameters must satisfy some known linear restrictions. These restrictions need to be satisfied both by (1) and (2). For example, the restrictions may state that a particular variable does not appear in one of the equation.\nIdentification means that (1) and (2) are the same: the only \\(F\\) which is allowed is a diagonal matrix. Why a diagonal matrix? Because when \\(F\\) is a diagonal matrix the coefficients of equation \\(i\\) in (2) are the same as the coefficients of equation \\(i\\) in (1) multiplied by a number, \\(f_i\\) say. Multiplying all the coefficients in the same equation by the same number is allowed because usually we single out one of the endogenous variables to have coefficient equal to one. This is the same as being interested in the ratios between each coefficients and the coefficient of this variables which is singled out.\nLet’s put all the coefficients of (1) in a matrix \\[A= \\left( \\begin{array}{c} B' \\\\ -\\Gamma' \\end{array}\\right)\\]. Notice that the coefficients of the first equation are in the first column of \\(A\\). We can partition \\(A\\) so that \\(A = (a_1,A_2)\\) where \\(a_1\\) is the $m+k $ vector of parameters in the first equation and \\(A_2\\) is a matrix containing all the parameters in the other \\(m-1\\) equations. We will focus on the first equation. Since we can change the order in which the equations are written, this does not involve any loss of generality.\nAs argued above we know that the restrictions satisfied by the parameters in the first equation take the linear form \\(R_1 a_1 =0\\) where \\(R_1\\) is a known \\((r_1 \\times m+k)\\) matrix of rank \\(r_1\\). Notice that if we multiply \\(a_1\\) by a constant \\(\\alpha \\ne 0\\), \\(R_1 (\\alpha a_1) =0\\) would still hold. So using this restriction we can only hope to identify \\(a_1\\) up to a multiplicative constant. Often, we require that one of the coefficients of \\(a_1\\) is 1 to fix such constant.\nLet \\(A^*= \\left( \\begin{array}{c} B' F'\\\\ -\\Gamma' F' \\end{array}\\right) =\\left( a_1^*, A_2^*\\right)\\) be a matrix containing the coefficients in (2). The we must also have \\(R_1 a_1^* =0\\).\nNotice that \\(A* =AF'\\) so that \\(\\left( a_1^*, A_2^*\\right)=\\left( a_1, A_2\\right)F'\\). Partition \\(F'\\) as\n\\[F'=\\left( \\begin{array}{cc}\nf_{11} & f_{12} \\\\ f_{21}&F_{22}\n\\end{array} \\right)\\] so that\n\\[\\left( a_1^*, A_2^*\\right)=\\left( a_1 f_{11} +A_2 f_{21},a_1 f_{12} + A_2 F_{22}\\right).\\]\nNow notice that\n\\[R_1 a_1^* = R_1 \\left( a_1 f_{11} +A_2 f_{21}\\right) = R_1 a_1 f_{11} +R_1 A_2 f_{21}\\]\nWe know that \\(R_1 a_1 =0\\) and \\(R_1 a_1^*=0\\) so that we must have\n\\(R_1 A_2 f_{21}=0.\\)\nThis last equation implies that \\(f_{21}=0\\) if and only if \\(R_1A_2\\) has rank full rank \\(m-1\\). This is the rank condition for identification of the coefficients of the first equation. If this condition holds, \\(a_1^*=a_1 f_{11}\\) so that \\(a_1^*\\) is a multiple of \\(a_1\\).\nWe can repeat this reasoning for all equations in (1) so that the coefficients of the whole system will be identified if and only if rank conditions hold for all equations. If Identification holds for the coefficients of all equations in the system then \\(F\\) is a diagonal matrix.\nThis proof is not mine. If I had to guess I would attribute it to Grant Hillier during my MSc in Southampton …. a few years ago."
  },
  {
    "objectID": "posts/PhdDefense/index.html",
    "href": "posts/PhdDefense/index.html",
    "title": "A PhD thesis defense",
    "section": "",
    "text": "Last Friday I attended my first PhD ‘defense’ in the Swedish system. It was definitely enjoyable and it was interesting to discover what Mattias, the ‘defendant’, has researched in the last few years.\nThe structure of the defense was as follows. Firstly the ‘opponent’, an established expert in the area, gave a general overview of the PhD research topic and put it into a context; secondly, he asked probing questions to the defendant - some of which were general and some very specific - which Mattias answer very confidently; thirdly the members of the grading committee - because the grade was decided by a three persons committee not by the opponent - and the public had the chance to ask further questions.\nThe defence was public. In fact it was attended by academics, colleagues, students, friends and family. It was a rite of passage very different from the UK viva - which I experienced directly and as an internal examiner- or by the Australian thesis submission - experienced through my PhD students.\nThe viva in the UK is a private matter between the candidate and the internal and external examiners. Although it can be certainly as intimidating as the ‘defence’, it is an individual rite of passage rather than a public one. I feel it reflects the the different ways in which education is perceived in the two countries: it is for the benefit of the individual in the UK but it is for the benefit of Society in Sweden. A fundamental difference.\nIn Australia the thesis is submitted and two external examiners write two independent reports on the thesis recommending the award of the PhD or a resubmission subject to either small corrections or, sometimes, fundamental changes and resubmission. There is no discussion with the examiners, and as a rite of passage, it is definitely subdued.\nAs a young non conformist PhD student the latter would have been my favourite approach. But I have changed my mind as I got older. I have come to realise that a few rites of passage are important - not so much for the individuals undergoing them - but for all the others who observe them in order to accept them as part of a group and to recognise their accomplishments. What I liked most of the defence on Friday was the fact that a layperson could have a glimpse into the research life of Mattias, and could see the hard work, determination, skills, knowledge, frustrations, pragmatism, leadership, independence and teamwork that completing a PhD requires - and that would be summarised by a recruiter as ‘proficient in stata’."
  },
  {
    "objectID": "posts/IdentificationVARS/index.html",
    "href": "posts/IdentificationVARS/index.html",
    "title": "Identification of Structural Vector Autoregressive Models",
    "section": "",
    "text": "In the last lecture for Econometrics B in the Economics MSc at Warwick we discussed structural vector autoregressive (SVAR) models. These are very similar to simultaneous equations but identification is achieved using slightly different conditions. In the lecture we discussed a couple of identification strategies for these models but we did not get into the details as these would have led us beyond what is been required in the MSc. However, a few students asked me for some more details. These are provided here.\nA SVAR model of order \\(p\\) is of the form:\n\\[\\Phi_0 y_t = \\Phi_1y_{t-1} + \\Phi_2y_{t-2} + ... + \\Phi_p y_{t-p} +\\varepsilon_t\\]\nwhere \\(y_t\\) is a vector of dimension \\(n\\). If one multiplies right- and left-hand-sides by the inverse of \\(\\Phi_0\\), assuming that it exists, a vector autoregressive model is obtained, corresponding to the reduced form of a SVAR model:\n\\[y_t = \\Phi_0^{-1}\\Phi_1y_{t-1} + \\Phi_0^{-1}\\Phi_2y_{t-2} + ... + \\Phi_0^{-1}\\Phi_p y_{t-p} + \\Phi_0^{-1}\\varepsilon_t\\]\n\\[= \\Psi_1 y_{t-1} + \\Psi_2 y_{t-2} + ... + \\Psi_p y_{t-p} + \\varepsilon_t^*\\]\nwhere \\(\\Psi_1=\\Phi_0^{-1} \\Phi_1\\) …. \\(\\Psi_p=\\Phi_0^{-1} \\Phi_p\\). The parameters \\(\\Psi_1\\) …. \\(\\Psi_p\\) are well defined as they are the coefficient matrices of a VAR model. Similarly, \\(var(\\varepsilon^{*}_t)\\) is also well defined.\nNotice that by defining \\(\\Phi_0=B\\) and \\(\\Gamma=\\left[ \\Phi_1,...,\\Phi_p\\right]\\) and collecting all the right-hand-side variables into a vector \\(z_t\\), then the SVAR model can be written as \\(By_t=\\Gamma z_t+\\varepsilon_t\\) and the reduced form is \\(y_t=B^{-1}\\Gamma z_t+B^{-1}\\varepsilon_t\\). This is the same model for which identification was discussed in a previous post using linear restrictions on the matrix \\((B,-\\Gamma)\\). However, identification of SVAR models is achieved not through linear restrictions on \\((B,-\\Gamma)\\) but by imposing restrictions on the variance-covariance matrix of \\(\\varepsilon_t\\) and \\(\\Phi_0\\).\nThe most important assumption for identification is that \\(var(\\varepsilon_t) =I_n\\) where \\(I_n\\) denotes an identity matrix (in general it can be any known positive definite matrix). Then, \\(var(\\varepsilon_t^*)=\\Phi_0^{-1} (\\Phi_0^{-1})'\\). Since \\(var(\\varepsilon_t^*)\\) is well defined, \\(\\Phi_0\\) is identified if it can be obtained uniquely from \\(var(\\varepsilon_t^*)=\\Phi_0^{-1} (\\Phi_0^{-1})'\\). One way to achieve identification in SVAR models is to appeal to the Choleski decomposition and assume that \\(\\Phi_0\\) - and therefore also \\(\\Phi_0^{-1}\\) - is lower triangular.\nAlternatively one can allow the variance-covariance matrix \\(Var(\\varepsilon_t)\\) to be a diagonal matrix - with unknown diagonal terms -, but then \\(\\Phi_0\\) is required to be lower triangular with diagonal elements equal to 1.\nAnother strategy requires imposing long run restrictions. The intuition is that in a stationary equilibrium\n\\(y_t=y_{t-1}=...=y_{t-p}=y\\)\nso that\n\\(\\displaystyle\\Phi_0 y =\\Phi_1y + \\Phi_2y + ... + \\Phi_p y\\)\nor\n\\(\\displaystyle\\left( \\Phi_0 -\\Phi_1-\\Phi_2 - ... - \\Phi_p\\right) y=0.\\)\nDefine \\(\\Phi= \\Phi_0 -\\Phi_1-\\Phi_2 - ... - \\Phi_p\\) so that one can write \\(\\Phi y=0.\\) That is \\(\\Phi\\) captures the long term relationships between the variables in \\(y\\). These restrictions can be exploited. To do this, notice that \\(\\Psi_1=\\Phi_0^{-1} \\Phi_1\\), …. \\(\\Psi_p=\\Phi_0^{-1} \\Phi_p\\) so that\n\\(I_m-\\Psi_1-...-\\Psi_p=\\Phi_0^{-1}\\left( \\Phi_0-\\Phi_1-...-\\Phi_p\\right).\\)\nLet \\(\\Psi =I_n-\\Psi_1-...-\\Psi_p\\) so that \\(\\Psi =\\Phi_0^{-1} \\Phi\\). Notice that \\(\\Psi\\) is well defined as it depends on the coefficient matrices of the VAR model. The restrictions on \\(\\Phi\\) plus the relationship \\(\\Psi =\\Phi_0^{-1} \\Phi\\) are not enough to determine \\(\\Phi_0\\) uniquely. However, there is another relationship which can be exploited: \\(var(\\varepsilon_t^*)=\\Phi_0^{-1} (\\Phi_0^{-1\\prime})\\). Notice that\\(var(\\varepsilon_t^*)\\) is well defined as it is the covariance matrix of the innovations in a VAR model."
  },
  {
    "objectID": "posts/CED1/index.html",
    "href": "posts/CED1/index.html",
    "title": "Computing exact distributions part 1",
    "section": "",
    "text": "The normal distribution is one of the basic distributions used in statistics. I occurs very often especially in asymptotic theory. All students of statistics and econometrics should be familiar with all its properties. In this section we generalize the normal distribution to the multivariate normal distribution. This is the joint distribution of a set of normal random variables that are not necessarily independent.\nIn this post I will try to:\n\nmake clear what is meant by mean vector and covariance matrix of a random vector;\nexplain what is meant by:\n\n\nmultivariate normal distribution,\nelliptically symmetric distribution,\nmean- and covariance matrix-mixed normal distribution, and\nspherically symmetric distribution;\n\n\nto use some of the properties of the distributions in (2);"
  },
  {
    "objectID": "posts/CED1/index.html#the-multivariate-normal-and-related-distributions",
    "href": "posts/CED1/index.html#the-multivariate-normal-and-related-distributions",
    "title": "Computing exact distributions part 1",
    "section": "",
    "text": "The normal distribution is one of the basic distributions used in statistics. I occurs very often especially in asymptotic theory. All students of statistics and econometrics should be familiar with all its properties. In this section we generalize the normal distribution to the multivariate normal distribution. This is the joint distribution of a set of normal random variables that are not necessarily independent.\nIn this post I will try to:\n\nmake clear what is meant by mean vector and covariance matrix of a random vector;\nexplain what is meant by:\n\n\nmultivariate normal distribution,\nelliptically symmetric distribution,\nmean- and covariance matrix-mixed normal distribution, and\nspherically symmetric distribution;\n\n\nto use some of the properties of the distributions in (2);"
  },
  {
    "objectID": "posts/CED1/index.html#multivariate-distributions-mean-vectors-and-variance-covariance-matrices",
    "href": "posts/CED1/index.html#multivariate-distributions-mean-vectors-and-variance-covariance-matrices",
    "title": "Computing exact distributions part 1",
    "section": "Multivariate distributions, mean vectors and variance-covariance matrices",
    "text": "Multivariate distributions, mean vectors and variance-covariance matrices\nA multivariate distribution is the joint distribution of two or more random variables.\nIn the same way univariate distribution are often summarized using means and variances, multivariate distributions are often summarized using mean vectors and variance-covariance matrices.\nFor the sake of simplicity we represent \\(n\\) random variables \\({x_1},{x_2},...,{x_n}\\) in the form of a column vector, \\(x = \\left( {{x_1},{x_2},...,{x_n}} \\right)'\\) say. Since the components of \\(x\\) are random, we say that \\(x\\) is a random vector.\nThe mean vector of an (\\(n \\times 1\\)) random vector \\(x = \\left( {{x_1},{x_2},...,{x_n}} \\right)'\\) is the (\\(n \\times 1\\)) vector of the means\n\\[E\\left( x \\right) = \\left( \\begin{array}{l}\nE\\left( {{x_1}} \\right)\\\\\nE\\left( {{x_2}} \\right)\\\\\n\\vdots \\\\\nE\\left( {{x_n}} \\right)\n\\end{array} \\right). \\tag{1}\\]\nNotice that the mean vector may not exist, because its component may not be defined.\nIt is easy to check that if \\(A\\) and \\(b\\) are respectively an (\\(m \\times n\\)) matrix and an (\\(m \\times 1\\)) vector of constants then \\[E\\left( {Ax + b} \\right) = AE\\left( x \\right) + b. \\tag{2}\\]\nSimilarly, we define an (\\(n \\times k\\)) random matrix \\(X\\) as a list of ordered random variables \\({x_{ij}}\\) witten in \\(n\\) rows and \\(k\\) columns. The mean matrix of \\(X\\) is a the (\\(n \\times k\\)) matrix \\(E\\left( X \\right)\\) containing the means of the respective components \\(E\\left( {{x_{ij}}} \\right)\\). Equation 2 generalizes to \\[E\\left( {AX + B} \\right) = AE\\left( X \\right) + B,\\] where \\(A\\) and \\(B\\) are (\\(m \\times n\\)) and (\\(m \\times k\\)) fixed matrices.\nIf the random vector \\(x\\) has mean \\(\\mu\\), the covariance matrix of \\(x\\) is defined to be the (\\(n \\times n\\)) matrix\n\\[\\Sigma  = {\\rm cov} \\left( x \\right) = E\\left[ {\\left( {x - \\mu } \\right)\\left( {x - \\mu } \\right)'} \\right]. \\tag{3}\\]\nNote that the element in position (i,j), \\({\\sigma _{ij}} = E\\left( {\\left( {{x_i} - {\\mu _i}} \\right)\\left( {{x_j} - {\\mu _j}} \\right)} \\right)\\), is the covariance between \\({x_i}\\) and \\({x_j}\\), and the element in position (i,i), \\({\\sigma _{ii}} = E\\left[ {{{\\left( {{x_i} - {\\mu _i}} \\right)}^2}} \\right]\\), is the variance of \\({x_i}\\).\nFor a matrix \\(X\\) a covariance matrix is defined as the covariance matrix of \\(vec(X)\\) obtained by stacking the columns of \\(X\\) one above the other.\nIt is common to assume that a covariance matrix is positive definite, that is for any (\\(n \\times 1\\)) vector \\(q \\ne 0\\) it follows that \\(q'\\Sigma q &gt; 0\\). This is essentially the multivariate version of the fact that a variance must be positive. Notice that we could allow a covariance matrix to be positive semi-definite (i.e. for any (\\(n \\times 1\\)) vector \\(q \\ne 0\\) it follows that \\(q'\\Sigma q \\ge 0\\)), but this would involve some sort of degeneracy like in the univariate case where the variance is zero. This in turn creates several complications.\nIf A and b are respectively an (\\(m \\times n\\)) and an (\\(m \\times 1\\)) matrix of constants then \\[{\\rm cov} \\left( {Ax + b} \\right) = A{\\mathop{\\rm cov}} \\left( x \\right)A', \\tag{4}\\] provided the resulting matrix is positive definite."
  },
  {
    "objectID": "posts/CED1/index.html#the-multivariate-normal-distribution",
    "href": "posts/CED1/index.html#the-multivariate-normal-distribution",
    "title": "Computing exact distributions part 1",
    "section": "The multivariate normal distribution",
    "text": "The multivariate normal distribution\nDefinition 1. Let \\(x\\) be an (\\(n \\times 1\\)) random vector. We say that \\(x\\) has a multivariate normal distribution with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\), and write \\(x \\sim N\\left( {\\mu ,\\Sigma } \\right)\\), if the joint density function of the components of \\(x\\) is \\[pdf\\left( {{x_1},...,{x_n}} \\right) = pdf\\left( x \\right) = {\\left( {2\\pi } \\right)^{ - n/2}}{\\left| \\Sigma  \\right|^{ - 1/2}}\\exp \\left\\{ { - \\frac{1}{2}\\left( {x - \\mu } \\right)'{\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right)} \\right\\} \\tag{5}\\] where \\(\\left| \\Sigma \\right|\\) denotes the determinant of \\(\\Sigma\\).\nNote that the density function of \\(x\\) has the same for values for \\(x\\) satisfying\n\\[\\left( {x - \\mu } \\right)'{\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right) = {\\rm{constant,}} \\tag{6}\\]\ni.e. it is constant on an ellipse. This suggests a generalization of the multivariate normal distribution. Instead of taking the exponential function we take another function \\(\\phi\\), and write \\[pdf\\left( x \\right) = c\\phi \\left[ {\\left( {x - \\mu } \\right)'{\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right)} \\right],\\] where \\(c\\) is a normalizing constant. This is the family of elliptically symmetric densities. Notice that \\(\\Sigma\\) is not the covariance matrix of \\(x\\) in this case. It affects the shape of the level sets and it is known as the shape parameter. The parameter \\(\\mu\\) is the mean vector of x (provided the \\(E\\left( x \\right)\\) exists). Notice that the function \\(\\phi\\) may be defined on a subset of of the form \\(\\left( {x - \\mu } \\right)'{\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right) \\le l \\le \\infty\\) only because of the condition that the density of \\(x\\) integrates to one.\nThe family of elliptically symmetric densities is important because all marginal distributions are also elliptically symmetric and their densities have the same functional forms.\nIf \\(\\Sigma = \\lambda {I_n}\\), then \\(\\left( {x - \\mu } \\right)'\\left( {x - \\mu } \\right) =\\)constant determines a circle or, more generally, a sphere. In this case the multivariate normal is a member to the family of spherically symmetric densities: \\[pdf\\left( x \\right) = c\\phi \\left[ {\\left( {x - \\mu } \\right)'\\left( {x - \\mu } \\right)} \\right].\\] Here, \\(c\\) is again a normalizing constant.\nSome members of elliptically symmetric and spherically symmetric family of densities can be generated as integrals of the multivariate normal distribution. We will see an example later.\nThe following result tells use how a normal vector changes under linear transformations.\nTheorem 1. Let \\(x \\sim N\\left( {\\mu ,\\Sigma } \\right)\\) and \\(A\\) and \\(b\\) be respectively an (\\(m \\times n\\))and an (\\(m \\times 1\\)) matrices of constants. Let \\(z = Ax + b\\) then \\(z \\sim N\\left( {A\\mu + b,A\\Sigma A'} \\right)\\).\nAn important application of Theorem 1 is the following:\nCorollary 1.1. Suppose that \\(A\\) is an \\(m \\times n\\) random matrix distributed independently of the \\(n \\times 1\\) random vector \\(x \\sim N\\left( {\\mu ,\\Sigma } \\right)\\). Let \\(z = Ax\\). Then, the distribution of \\(z\\) conditional on \\(A\\) is \\[z|A \\sim N\\left( {A\\mu ,A\\Sigma A'} \\right).\\]\nNotice that the joint density of \\(z\\) and \\(A\\) is \\[pdf\\left( {z,A} \\right) = pdf\\left( {z|A} \\right) \\times pdf\\left( A \\right),\\] which implies that \\[pdf(z)= \\int_{\\mathbb{R}^{m \\times n}} N(A\\mu,A\\Sigma A') pdf(A)dA.\\]\nIn this case, we say that \\(z\\) is a mean-and-covariance-matrix mixture of a multivariate normal with mixing density \\(pdf\\left( A \\right)\\).\nCorollary 1.2. Suppose that \\(A\\) is an \\(m \\times n\\)random matrix and that the \\(n \\times 1\\) random vector \\(x|A \\sim N\\left( {\\mu \\left( A \\right),\\Sigma \\left( A \\right)} \\right)\\). Let \\(z = Ax\\). Then, for fixed A, one has \\[z|A \\sim N\\left( {A\\mu \\left( A \\right),A\\Sigma \\left( A \\right)A'} \\right).\\]\nAs before the joint density of \\(\\left( {z,A} \\right)\\) is \\(pdf\\left( {z,A} \\right) = pdf\\left( {z|A} \\right) \\times pdf\\left( A \\right)\\) so that \\[pdf(z)= \\int_{\\mathbb{R}^{m \\times n}} N(A\\mu(A),A\\Sigma(A) A') pdf(A)dA.\\]\nOnce again, $\\(z\\) is said to be a mean-and-covariance-matrix mixture of a multivariate normal with mixing density \\(pdf\\left( A \\right)\\).\nExample 1. Let \\(y|X \\sim N\\left( {X\\beta ,{\\sigma ^2}{I_n}} \\right)\\) where $$ has is a vector of dimensions \\(k \\times 1\\). This is a linear regression model \\(y = X\\beta + u\\) where \\(u|X \\sim N\\left( {0,{\\sigma ^2}{I_n}} \\right)\\). Let \\({\\hat \\beta _n} = {\\left( {X'X} \\right)^{ - 1}}X'y\\) be the OLS estimator of \\(\\beta\\), and apply Corollary 2 with \\(A = {\\left( {X'X} \\right)^{ - 1}}X'\\): \\[{\\hat \\beta _n}|X = {\\hat \\beta _n}|X'X\\sim N\\left( {\\beta ,{\\sigma ^2}{{\\left( {X'X} \\right)}^{ - 1}}} \\right).\\] Note that \\[\\begin{array}{c}\npdf\\left( {{{\\hat \\beta }_n}|X} \\right) = pdf\\left( {{{\\hat \\beta }_n}|X'X} \\right)\\\\\n= {\\left( {2\\pi {\\sigma ^2}} \\right)^{ - k/2}}{\\left| {X'X} \\right|^{1/2}}\\exp \\left\\{ { - \\frac{1}{{2{\\sigma ^2}}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right)'X'X\\left( {{{\\hat \\beta }_n} - \\beta } \\right)} \\right\\}\n\\end{array}\\]\nThus, integrating over the space \\(X'X &gt; 0\\) we have\n\\[\\begin{array}{c}\npdf\\left( {{{\\hat \\beta }_n}} \\right) = \\int\\limits_{X'X &gt; 0} {pdf\\left( {{{\\hat \\beta }_n}|X'X} \\right)pdf\\left( {X'X} \\right)d\\left( {X'X} \\right)} \\\\\n= {\\left( {2\\pi {\\sigma ^2}} \\right)^{ - k/2}}\\int\\limits_{X'X &gt; 0} {{{\\left| {X'X} \\right|}^{1/2}}\\exp \\left\\{ { - \\frac{1}{{2{\\sigma ^2}}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right)'X'X\\left( {{{\\hat \\beta }_n} - \\beta } \\right)} \\right\\}} pdf\\left( {X'X} \\right)d\\left( {X'X} \\right)\\\\\n= \\phi \\left[ {\\frac{1}{{2{\\sigma ^2}}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right)'\\left( {{{\\hat \\beta }_n} - \\beta } \\right)} \\right]\n\\end{array}\\]\nso that \\({\\hat \\beta _n}\\) has a elliptically symmetric distribution.\nIn this example, \\(\\phi\\) is obtained by evaluating the integral above. If we are interested in a sub-vector \\({\\hat \\beta _{1n}}\\) of \\({\\hat \\beta _n}\\), we can conclude that \\({\\hat \\beta _{1n}}\\) has an elliptically symmetric distribution of the form\n\\[\\phi \\left[ {\\frac{1}{{2{\\sigma ^2}}}\\left( {{{\\hat \\beta }_{1n}} - {\\beta _1}} \\right)'\\left( {{{\\hat \\beta }_{1n}} - {\\beta _1}} \\right)} \\right].\\]\nThis result fits in with the standard asymptotics for the linear regression model. It is usually assumed that \\({n^{ - 1}}X'X{ \\to ^P}Q\\) as \\(n \\to \\infty\\) where \\(Q\\) is a positive definite matrix. Then\n\\[{\\hat \\beta _n}|{n^{ - 1}}X'X\\sim N\\left( {\\beta ,{n^{ - 1}}{\\sigma ^2}{{\\left( {{n^{ - 1}}X'X} \\right)}^{ - 1}}} \\right)\\]\nSo that, by rearranging, one obtains\n\\[{n^{1/2}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right)|{n^{ - 1}}X'X \\sim N\\left( {0,{\\sigma ^2}{{\\left( {{n^{ - 1}}X'X} \\right)}^{ - 1}}} \\right).\\]\nAs \\(n \\to \\infty\\), \\({n^{ - 1}}X'X\\) becomes \\(Q\\), a non-random quantity, so that\n\\[{n^{1/2}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right)|{n^{ - 1}}X'X{ \\to ^L}{n^{1/2}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right)|Q = N\\left( {0,{\\sigma ^2}{Q^{ - 1}}} \\right).\\]\nSince \\(Q\\) is fixed\n\\[{n^{1/2}}\\left( {{{\\hat \\beta }_n} - \\beta } \\right){ \\to ^L}N\\left( {0,{\\sigma ^2}{Q^{ - 1}}} \\right).\\] This is a standard result for the linear regression model.\nTheorem 2. Let \\(x\\sim N\\left( {\\mu ,\\Sigma } \\right)\\). Partition \\(x = \\left( \\begin{array}{l} {x_1}\\\\ {x_2} \\end{array} \\right)\\begin{array}{*{20}{c}} {\\} {n_1}}\\\\ {\\} {n_2}} \\end{array}\\) , \\(\\mu = \\left( \\begin{array}{l} {\\mu _1}\\\\ {\\mu _2} \\end{array} \\right)\\begin{array}{*{20}{c}} {\\} {n_1}}\\\\ {\\} {n_2}} \\end{array}\\) and \\[\\Sigma  = \\left( {\\begin{array}{*{20}{c}}\n\\begin{array}{l}\n{\\Sigma _{11}}\\\\\n{n_1} \\times {n_1}\n\\end{array}&\\begin{array}{l}\n{\\Sigma _{21}}'\\\\\n{n_1} \\times {n_2}\n\\end{array}\\\\\n\\begin{array}{l}\n{\\Sigma _{21}}\\\\\n{n_2} \\times {n_1}\n\\end{array}&\\begin{array}{l}\n{\\Sigma _{22}}\\\\\n{n_2} \\times {n_2}\n\\end{array}\n\\end{array}} \\right).\\] Then \\[\\begin{array}{l}\n{x_1}\\sim N\\left( {{\\mu _1},{\\Sigma _{11}}} \\right)\\\\\n{x_2}\\sim N\\left( {{\\mu _2},{\\Sigma _{22}}} \\right)\\\\\n{x_1}|{x_2}\\sim N\\left( {{\\mu _1} + {\\Sigma _{21}}'\\Sigma _{22}^{ - 1}\\left( {{x_2} - {\\mu _2}} \\right),{\\Sigma _{11.2}}} \\right)\n\\end{array}\\]\nwhere \\({\\Sigma _{11.2}} = {\\Sigma _{11}} - {\\Sigma _{21}}'\\Sigma _{22}^{ - 1}{\\Sigma _{21}}\\).\nThe proof of Theorem 2 relies on two classical results for partitioned matrices: \\[\\left| \\Sigma  \\right| = \\left| {{\\Sigma _{11}}} \\right|\\left| {{\\Sigma _{11.2}}} \\right| \\tag{7}\\]\nand \\[{\\left( {\\begin{array}{*{20}{c}}\n{{\\Sigma _{11}}}&{{\\Sigma _{21}}'}\\\\\n{{\\Sigma _{21}}}&{{\\Sigma _{22}}}\n\\end{array}} \\right)^{ - 1}} = \\left( {\\begin{array}{*{20}{c}}\n{{A_{11}}}&{{A_{12}}}\\\\\n{{A_{21}}}&{{A_{22}}}\n\\end{array}} \\right) = \\left( {\\begin{array}{*{20}{c}}\n{\\Sigma _{11.2}^{ - 1}}&{ - \\Sigma _{11}^{ - 1}{\\Sigma _{21}}'\\Sigma _{22.1}^{ - 1}}\\\\\n{ - \\Sigma _{22.1}^{ - 1}{\\Sigma _{21}}\\Sigma _{11}^{ - 1}}&{\\Sigma _{22.1}^{ - 1}}\n\\end{array}} \\right), \\tag{8}\\]\nwhere \\({\\Sigma _{22.1}} = {\\Sigma _{22}} - {\\Sigma _{21}}\\Sigma _{11}^{ - 1}{\\Sigma _{21}}'\\). Define \\(A = {\\Sigma ^{ - 1}}\\) and \\(y = x - \\mu\\), then, partitioning \\(y\\) and \\(A\\) conformably to \\(x\\), we have\n\\[\\begin{array}{c}\ny'Ay = {y_2}'{A_{22}}{y_2} + \\left( {{y_1} + A_{11}^{ - 1}{A_{12}}{y_2}} \\right)'{A_{11}}\\left( {{y_1} + A_{11}^{ - 1}{A_{12}}{y_2}} \\right) - {y_2}'{A_{12}}'A_{11}^{ - 1}{A_{12}}{y_2}\\\\\n= {y_2}'\\left( {{A_{22}} - {A_{12}}'A_{11}^{ - 1}{A_{12}}} \\right){y_2} + \\left( {{y_1} + A_{11}^{ - 1}{A_{12}}{y_2}} \\right)'{A_{11}}\\left( {{y_1} + A_{11}^{ - 1}{A_{12}}{y_2}} \\right)\\\\\n= {y_2}'\\Sigma _{22}^{ - 1}{y_2} + \\left( {{y_1} - {\\Sigma _{12}}\\Sigma _{22}^{ - 1}{y_2}} \\right)'\\Sigma _{11.2}^{ - 1}\\left( {{y_1} - {\\Sigma _{12}}\\Sigma _{22}^{ - 1}{y_2}} \\right).\n\\end{array}\\]\nReplacing these in Equation 5 \\[\\begin{array}{c}\npdf\\left( {{x_1},{x_2}} \\right) = {\\left( {2\\pi } \\right)^{ - {n_1}/2}}{\\left| {{\\Sigma _{11}}} \\right|^{ - 1/2}}\\exp \\left\\{ { - \\frac{1}{2}\\left( {{x_2} - {\\mu _2}} \\right)'\\Sigma _{22}^{ - 1}\\left( {{x_2} - {\\mu _2}} \\right)} \\right\\} \\times \\\\\n{\\left( {2\\pi } \\right)^{ - {n_2}/2}}{\\left| {{\\Sigma _{11.2}}} \\right|^{ - 1/2}}\\exp \\left\\{ { - \\frac{1}{2}\\left( {{x_1} - {\\mu _1} - {\\Sigma _{12}}\\Sigma _{22}^{ - 1}\\left( {{x_2} - {\\mu _2}} \\right)} \\right)'\\Sigma _{22.1}^{ - 1}\\left( {{x_1} - {\\mu _1} - {\\Sigma _{12}}\\Sigma _{22}^{ - 1}\\left( {{x_2} - {\\mu _2}} \\right)} \\right)} \\right\\}\n\\end{array}\\] so that \\[{x_2}\\sim N\\left( {{\\mu _2},{\\Sigma _{22}}} \\right)\\] and \\[{x_1}|{x_2}\\sim N\\left( {{\\mu _1} + {\\Sigma _{21}}'\\Sigma _{22}^{ - 1}\\left( {{x_2} - {\\mu _2}} \\right),{\\Sigma _{11.2}}} \\right).\\]\nNotice that \\({\\Sigma _{11.2}}\\) does not depend on \\({x_2}\\), so that the fact that we condition on \\({x_2}\\) affects only the mean of the conditional distribution of \\({x_1}\\) given \\({x_2}\\). The fact that \\[{x_1}\\sim N\\left( {{\\mu _1},{\\Sigma _{11}}} \\right)\\] follows in the same way. The proof is complete.\nCorollary 2.1. Let \\(x\\) be as in Theorem 2. Then, \\({x_1}\\) and \\({x_2}\\) are independent if and only if \\({\\Sigma _{21}} = 0\\).\nThis is the case because if \\({\\Sigma _{21}} = 0\\) then \\[{x_1}|{x_2}\\sim N\\left( {{\\mu _1},{\\Sigma _{11}}} \\right)\\] and this is the marginal distribution of \\({x_1}\\).\nCorollary 2.2. Let \\(x\\sim N\\left( {\\mu ,\\Sigma } \\right)\\). Let \\({z_1} = {A_1}x\\) and \\({z_2} = {A_2}x\\) where \\({A_1}\\) and \\({A_2}\\) are \\({k_1} \\times n\\) and \\({k_2} \\times n\\) (\\({k_1} + {k_2} \\le n\\)) fixed matrices. Then \\({z_1}\\) and \\({z_2}\\) are independent if and only if \\({A_1}\\Sigma {A_2}' = 0\\).\nTo see this point one writes \\(z = \\left( \\begin{array}{l} {z_1}\\\\ {z_2} \\end{array} \\right)\\), calculates its covariance matrix and applies Corollary 2.1.\nThe multivariate normal distribution is used to define other distribution including the chi-square and Wishart distribution covered in the next post."
  },
  {
    "objectID": "posts/troubles/index.html",
    "href": "posts/troubles/index.html",
    "title": "The trouble with doing statistics without understanding it",
    "section": "",
    "text": "The average economics student takes at least one module in econometrics as an undergraduate. Master students normally take further econometric modules - now mostly covering microeconometrics (i.e. linear regression in cross-sections and panel data, dummy dependent variable models like logit, probit, and censored and truncated regression).\nSince individuals are heterogeneous, one would expect heteroskedasticity both in cross-section and in panel data. Most lecturers tell their students to use robust standard errors in applications using linear models. This is now very easy to do in practice because such procedures have been coded up in several computer packages. For example, in my modules I tend to use Stata, which offers the robust or the cluster options.\nAs lecturers, we tend to repeat this mantra so often that our students - and sometimes even researchers - report heteroskedasticity robust standard errors even in cases where this does not make sense.\nHeteroskedasticity robust standard errors for an estimator make sense when two conditions are satisfied: (1) the estimator is consistent even in the presence of heteroskedasticity and (2) the standard errors routinely reported by the statistical package are incorrectly calculated under heteroskedasticity. Hence, one needs to  understand the effects that heteroskedasticity on the consistency of an estimator before deciding whether heteroskedasticity robust standard errors should be reported.\nIn some situations heteroskedasticity fundamentally affects an estimator by making it inconsistent. For example the standard estimator for the Tobit model, a common model for censored regressions, is inconsistent under heteroskedasticity (see Arabmazar, A. and P. Schmidt (1981) ‘Further evidence on the robustness of the Tobit estimator to heteroskedasticity’, Journal of Econometrics 17, 253-258). Despite this, it is easy to report robust standard errors for the Tobit model since the usual robust or clustered options can be ticked. By doing this, one does not solve the essential problem of inconsistency of the estimator and may also give the researchers the impression that they have actually ‘dealt with’ heteroskedasticity when in fact they do not.\nIf well trained students and researchers can easily fall into this trap, what happens when these tools are used by individuals without training? This is more than a speculative worry. There are lots of tools on the market that allow individuals to ‘bypass’ the experts - those who know how the methods actually work - and perform analytics or machine learning with no knowledge of statistics."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am professor of Econometrics at Department of Economics of Umeå University in Sweden. Before joining Umeå, I spent five years in the Department of Economics at the University of Surrey (UK), five years in the Department of Econometrics and Business Statistics at Monash University (Australia) and seven years in the Department of Economics and Related Studies at the University of York (UK). I visited the Department of Economics at the University of Warwick between 2016 and 2018, where I taught the second semester of the advanced econometrics module in the MSc, and the School of Public Health, Imperial College London, between 2020-2022.\nApart from being an academic, I enjoy reading mathematics and statistics and using these to solve challenging real-world problems. I also engage in freelance statistical and econometrics consulting."
  }
]